{"name":"ADE","tagline":"Anomaly Detection Engine for Linux Logs","body":"# Anomaly detection Engine for Linux Logs (ADE) \r\n# Running ADE\r\n\r\nTo run ADE to detect anomalies in Linux Logs\r\nrequires both running ADE commands and examining\r\nthe Linux Logs to correctly select the time periods \r\nfor anomaly detection and the grouping of Linux systems.\r\n\r\n## Basic approach\r\n\r\nThe basic approach used by ADE is\r\n\r\n- pick the data to use to prime the model\r\n- group systems into groups\r\n- prime the database with Linux Logs; \r\n- create a model of normal behaviour from a set of Linux Logs\r\n- analyze additional logs to detect anomalies\r\n- examine the results written to the file system using your favorite web browser \r\n\r\n### Picking data to prime the model\r\nFor ADE to create a model that will generate useful analytic results, there\r\nneeds to be a sufficient number of unique message ids (message keys). Because ADE \r\ndoes unsupervised learning, ADE does not require the user to label either messages\r\nor intervals, it requires that the systems being analyzed are \"relatively\" stable.\r\n\r\n_**Almost any Linux system which is used to support production will be stable \r\nenough for ADE to find anomalies.**_\r\n\r\n### Determining how to group systems\r\nADE allows you to group similar systems together when building the model.\r\nHere is an example of nine servers and one way you can assign them to model groups:\r\n\r\n- model group 1\r\n   - primary mail server\r\n   - secondary mail server to handle traffic which overflows from the primary server\r\n   - deployment mail server is the Linux image on which a new version of the mail server code is deployed\r\n- model group 2\r\n   - external web server 1\r\n   - external web server 2\r\n   - external web server 3\r\n- model group 3\r\n   - internal web server 1\r\n   - internal web server 2\r\n   - internal web server 3\r\n  \r\n## Examine a time period to find if unusual behaviour occurred during the time period  \r\n\r\n### Priming the database\r\nTo prime the ADE database:\r\n- delete any information left in the database **controldb delete**\r\n- identify when the potential anomaly occurred \r\n- load Linux logs from time period immediately before that time period **Upload -f _filename or directoryname_**\r\n\r\n- check if sufficient data has been loaded **VerifyLinuxLogs _model group name_**\r\n   - if there is sufficient data then proceed to training\r\n   - else\r\n       - if there are additional logs from before the set of logs loaded then load\r\nadditional Linux logs\r\n       - if there aren't any more logs available then proceed to training - **results need to be\r\ncarefully reviewed** \t\r\n   \r\n### Creating a model\r\nTo create a model of the normal behaviour of the Linux systems:\r\n- issue Train command for each model group **Train _model group name_**\r\n\r\n### Create analysis results \r\nTo analyze the time period for anomalies\r\n- issue Analyze command for the time periods of interest **Analyze -f _filename or directoryname_**\r\n\r\n### Examine results\r\nTo examine the results point you web browser point at the file of interest.  Here is \r\nthe how the outputs are written to files\r\n\r\n- directory system_name 1\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n- directory system_name 2\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n    - directory yearMonthDay\r\n\t    - index.xml (summary of intervals within the period)\r\n\t\t- directory intervals\r\n\t\t    - interval_nnn.xml ( details of messages issued during this interval)\r\n\t\t\t- interval_nnn_debug.xml.gz ( information to debug problems with scorers (gzipped))\r\n- directory system_name 3\r\n\r\n## Continuously process Linux Logs to find unusual time periods\r\n\r\n### Priming the database\r\nTo prime the ADE database:\r\n- load Linux logs from time period immediately before today **Upload -f _filename or directoryname_**\r\n\r\n- check if sufficient data has been loaded **VerifyLinuxLogs _model group name_**\r\n   - if there is sufficient data then proceed to training\r\n   - else\r\n       - if there are additional logs from before the set of logs loaded then load\r\nadditional Linux logs\r\n       - if there aren't any more logs available then proceed to training - **continue to load\r\nload logs when they become available** \t\r\n   \r\n### Creating a model\r\nTo create a model of the normal behaviour of the Linux systems:\r\n- issue Train command for each model group **Train _model group name   start date   end date_ **\r\n- the suggested Training interval is 120 days and training should be done every 30 days\r\n\r\n### Create analysis results \r\nRoutinely analyze the available logs so anomalies information is available \r\n- issue Analyze command for the time periods of interest **Analyze -f _filename or directoryname_**\r\n\r\n### Examine results\r\nTo examine the results point you web browser point at the file of interest. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}